{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMAbRFBkRxO71PCfOLVYPWU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ayushomega14/Ayushomega14/blob/main/Stock_Market_Trend_Prediction_Comparison.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install arch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsMkgCePdQYx",
        "outputId": "175834c4-6a7f-4e00-ea28-48089ee18d23"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting arch\n",
            "  Downloading arch-7.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from arch) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.8 in /usr/local/lib/python3.10/dist-packages (from arch) (1.13.1)\n",
            "Requirement already satisfied: pandas>=1.4 in /usr/local/lib/python3.10/dist-packages (from arch) (2.2.2)\n",
            "Requirement already satisfied: statsmodels>=0.12 in /usr/local/lib/python3.10/dist-packages (from arch) (0.14.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4->arch) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4->arch) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4->arch) (2024.2)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.12->arch) (0.5.6)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.12->arch) (24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.6->statsmodels>=0.12->arch) (1.16.0)\n",
            "Downloading arch-7.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (985 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m985.1/985.1 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: arch\n",
            "Successfully installed arch-7.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-UKGWI5cxaT",
        "outputId": "03e2f876-0c4b-4f69-d246-3e9908b8e7e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 226ms/step - loss: 1.7908e-04 - mae: 0.0076 - val_loss: 4.0077e-04 - val_mae: 0.0131\n",
            "Epoch 2/100\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 264ms/step - loss: 1.2683e-05 - mae: 0.0022 - val_loss: 7.0875e-04 - val_mae: 0.0189\n",
            "Epoch 3/100\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 208ms/step - loss: 1.2361e-05 - mae: 0.0022 - val_loss: 3.2713e-04 - val_mae: 0.0101\n",
            "Epoch 4/100\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 226ms/step - loss: 1.3772e-05 - mae: 0.0022 - val_loss: 4.7480e-04 - val_mae: 0.0141\n",
            "Epoch 5/100\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 195ms/step - loss: 1.3837e-05 - mae: 0.0024 - val_loss: 8.2071e-04 - val_mae: 0.0228\n",
            "Epoch 6/100\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 194ms/step - loss: 9.8603e-06 - mae: 0.0020 - val_loss: 3.4695e-04 - val_mae: 0.0123\n",
            "Epoch 7/100\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 195ms/step - loss: 1.1834e-05 - mae: 0.0022 - val_loss: 7.0997e-04 - val_mae: 0.0214\n",
            "Epoch 8/100\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 193ms/step - loss: 1.0369e-05 - mae: 0.0019 - val_loss: 3.0406e-04 - val_mae: 0.0113\n",
            "Epoch 9/100\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 195ms/step - loss: 1.1083e-05 - mae: 0.0021 - val_loss: 2.6864e-04 - val_mae: 0.0094\n",
            "Epoch 10/100\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 214ms/step - loss: 8.9106e-06 - mae: 0.0019 - val_loss: 3.2598e-04 - val_mae: 0.0108\n",
            "Epoch 11/100\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 198ms/step - loss: 9.0851e-06 - mae: 0.0018 - val_loss: 3.1668e-04 - val_mae: 0.0117\n",
            "Epoch 12/100\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 197ms/step - loss: 9.4493e-06 - mae: 0.0021 - val_loss: 2.5563e-04 - val_mae: 0.0092\n",
            "Epoch 13/100\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 196ms/step - loss: 1.2001e-05 - mae: 0.0023 - val_loss: 2.7055e-04 - val_mae: 0.0106\n",
            "Epoch 14/100\n",
            "\u001b[1m  1/108\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35:33\u001b[0m 20s/step - loss: 8.3642e-06 - mae: 0.0022"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import statsmodels.api as sm\n",
        "from arch import arch_model\n",
        "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
        "import xgboost as xgb\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "file_path = '/content/drive/My Drive/bajaj_stock_data.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Ensure the Date column is in the correct format and set as index\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df.set_index('Date', inplace=True)\n",
        "\n",
        "# Drop NaN values\n",
        "df = df.dropna()\n",
        "\n",
        "# Scale the features 'Close', 'Open', 'High', 'Low', and 'Volume'\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled_data = scaler.fit_transform(df[['Close', 'Open', 'High', 'Low', 'Volume']])\n",
        "\n",
        "# Create datasets with multiple features (using 'Close', 'Open', 'High', 'Low', 'Volume')\n",
        "def create_dataset_multifeature(dataset, look_back=120):\n",
        "    X, y = [], []\n",
        "    for i in range(look_back, len(dataset)):\n",
        "        X.append(dataset[i-look_back:i])  # Include all features\n",
        "        y.append(dataset[i, 0])  # Predict 'Close' price (index 0)\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Increase the look-back window to capture longer trends\n",
        "look_back = 120\n",
        "X, y = create_dataset_multifeature(scaled_data, look_back)\n",
        "\n",
        "# Reshape X to be [samples, time steps, features]\n",
        "X = np.reshape(X, (X.shape[0], X.shape[1], X.shape[2]))\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
        "\n",
        "# Build the LSTM model with increased complexity\n",
        "model = Sequential()\n",
        "model.add(LSTM(units=100, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "model.add(Dropout(0.3))  # Adjust dropout for regularization\n",
        "model.add(LSTM(units=100, return_sequences=False))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(units=50))  # Extra Dense layer for more complexity\n",
        "model.add(Dense(units=1))  # Output layer\n",
        "\n",
        "# Compile the model with a lower learning rate\n",
        "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
        "\n",
        "# Train the model with early stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(X_train, y_train, batch_size=32, epochs=100, validation_split=0.2, callbacks=[early_stopping])\n",
        "\n",
        "# Predictions and inverse transformation\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Inverse transformation for predictions\n",
        "predictions = scaler.inverse_transform(\n",
        "    np.concatenate((predictions, np.zeros((predictions.shape[0], X_test.shape[2]-1))), axis=1)\n",
        ")[:, 0]\n",
        "\n",
        "# Inverse transform y_test for comparison\n",
        "y_test_scaled = scaler.inverse_transform(\n",
        "    np.concatenate((y_test.reshape(-1, 1), np.zeros((y_test.shape[0], X_test.shape[2]-1))), axis=1)\n",
        ")[:, 0]\n",
        "\n",
        "# --- ARIMA Model ---\n",
        "arima_order = (5, 1, 0)  # (p, d, q)\n",
        "arima_model = sm.tsa.ARIMA(df['Close'][:len(X_train) + look_back], order=arima_order)\n",
        "arima_fitted = arima_model.fit()\n",
        "arima_predictions = arima_fitted.forecast(steps=len(X_test))\n",
        "\n",
        "# Convert ARIMA predictions from Series to NumPy array for scaling\n",
        "arima_predictions_np = np.array(arima_predictions).reshape(-1, 1)\n",
        "\n",
        "# Inverse scale the ARIMA predictions\n",
        "arima_predictions_scaled = scaler.inverse_transform(\n",
        "    np.concatenate((arima_predictions_np, np.zeros((arima_predictions_np.shape[0], X_test.shape[2]-1))), axis=1)\n",
        ")[:, 0]\n",
        "\n",
        "# --- GARCH Model ---\n",
        "garch_model = arch_model(df['Close'][:len(X_train) + look_back], vol='Garch', p=1, q=1)\n",
        "garch_fitted = garch_model.fit(disp=\"off\")\n",
        "garch_forecast = garch_fitted.forecast(horizon=len(X_test))\n",
        "garch_predictions = garch_forecast.mean.values[-1, :]\n",
        "\n",
        "# Inverse scale the GARCH predictions\n",
        "garch_predictions_scaled = scaler.inverse_transform(\n",
        "    np.concatenate((garch_predictions.reshape(-1, 1), np.zeros((garch_predictions.shape[0], X_test.shape[2]-1))), axis=1)\n",
        ")[:, 0]\n",
        "\n",
        "# --- Exponential Smoothing (ETS) Model ---\n",
        "ets_model = ExponentialSmoothing(df['Close'][:len(X_train) + look_back], trend='add', seasonal='add', seasonal_periods=12)\n",
        "ets_fitted = ets_model.fit()\n",
        "ets_predictions = ets_fitted.forecast(steps=len(X_test))\n",
        "\n",
        "# Convert ETS predictions to NumPy array for scaling\n",
        "ets_predictions_np = np.array(ets_predictions).reshape(-1, 1)\n",
        "\n",
        "# Inverse scale the ETS predictions\n",
        "ets_predictions_scaled = scaler.inverse_transform(\n",
        "    np.concatenate((ets_predictions_np, np.zeros((ets_predictions_np.shape[0], X_test.shape[2]-1))), axis=1)\n",
        ")[:, 0]\n",
        "\n",
        "# --- XGBoost Model ---\n",
        "xgb_model = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1)\n",
        "xgb_model.fit(X_train.reshape(X_train.shape[0], -1), y_train)\n",
        "xgb_predictions = xgb_model.predict(X_test.reshape(X_test.shape[0], -1))\n",
        "\n",
        "# Inverse scale the XGBoost predictions\n",
        "xgb_predictions_scaled = scaler.inverse_transform(\n",
        "    np.concatenate((xgb_predictions.reshape(-1, 1), np.zeros((xgb_predictions.shape[0], X_test.shape[2]-1))), axis=1)\n",
        ")[:, 0]\n",
        "\n",
        "# Plot each model against LSTM\n",
        "plt.figure(figsize=(16, 8))\n",
        "\n",
        "# --- ARIMA vs LSTM ---\n",
        "plt.subplot(5, 1, 1)\n",
        "plt.plot(df.index[-len(y_test):], y_test_scaled, label='Actual Price', color='orange')\n",
        "plt.plot(df.index[-len(y_test):], predictions, label='LSTM Predictions', color='green', linewidth=2)\n",
        "plt.plot(df.index[-len(y_test):], arima_predictions_scaled, label='ARIMA Predictions', color='red')\n",
        "plt.title('ARIMA vs LSTM')\n",
        "plt.legend()\n",
        "\n",
        "# --- GARCH vs LSTM ---\n",
        "plt.subplot(5, 1, 2)\n",
        "plt.plot(df.index[-len(y_test):], y_test_scaled, label='Actual Price', color='orange')\n",
        "plt.plot(df.index[-len(y_test):], predictions, label='LSTM Predictions', color='green', linewidth=2)\n",
        "plt.plot(df.index[-len(y_test):], garch_predictions_scaled, label='GARCH Predictions', color='blue')\n",
        "plt.title('GARCH vs LSTM')\n",
        "plt.legend()\n",
        "\n",
        "# --- ETS vs LSTM ---\n",
        "plt.subplot(5, 1, 3)\n",
        "plt.plot(df.index[-len(y_test):], y_test_scaled, label='Actual Price', color='orange')\n",
        "plt.plot(df.index[-len(y_test):], predictions, label='LSTM Predictions', color='green', linewidth=2)\n",
        "plt.plot(df.index[-len(y_test):], ets_predictions_scaled, label='ETS Predictions', color='purple')\n",
        "plt.title('ETS vs LSTM')\n",
        "plt.legend()\n",
        "\n",
        "# --- XGBoost vs LSTM ---\n",
        "plt.subplot(5, 1, 4)\n",
        "plt.plot(df.index[-len(y_test):], y_test_scaled, label='Actual Price', color='orange')\n",
        "plt.plot(df.index[-len(y_test):], predictions, label='LSTM Predictions', color='green', linewidth=2)\n",
        "plt.plot(df.index[-len(y_test):], xgb_predictions_scaled, label='XGBoost Predictions', color='brown')\n",
        "plt.title('XGBoost vs LSTM')\n",
        "plt.legend()\n",
        "\n",
        "# --- All models together ---\n",
        "plt.subplot(5, 1, 5)\n",
        "plt.plot(df.index[-len(y_test):], y_test_scaled, label='Actual Price', color='orange')\n",
        "plt.plot(df.index[-len(y_test):], predictions, label='LSTM Predictions', color='green', linewidth=2)\n",
        "plt.plot(df.index[-len(y_test):], arima_predictions_scaled, label='ARIMA Predictions', color='red')\n",
        "plt.plot(df.index[-len(y_test):], garch_predictions_scaled, label='GARCH Predictions', color='blue')\n",
        "plt.plot(df.index[-len(y_test):], ets_predictions_scaled, label='ETS Predictions', color='purple')\n",
        "plt.plot(df.index[-len(y_test):], xgb_predictions_scaled, label='XGBoost Predictions', color='brown')\n",
        "plt.title('All Models vs LSTM')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Evaluation metrics for each model\n",
        "models = {'ARIMA': arima_predictions_scaled, 'GARCH': garch_predictions_scaled, 'ETS': ets_predictions_scaled, 'XGBoost': xgb_predictions_scaled}\n",
        "for model_name, model_preds in models.items():\n",
        "    mse = mean_squared_error(y_test_scaled, model_preds)\n",
        "    mae = mean_absolute_error(y_test_scaled, model_preds)\n",
        "    print(f\"{model_name} - MSE: {mse}, MAE: {mae}\")\n",
        "\n",
        "# LSTM evaluation already computed\n",
        "print(f\"LSTM - MSE: {mean_squared_error(y_test_scaled, predictions)}, MAE: {mean_absolute_error(y_test_scaled, predictions)}\")"
      ]
    }
  ]
}